{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Pre-Processing"
      ],
      "metadata": {
        "id": "btvQRijKpT9O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive', force_remount = True)\n",
        "\n",
        "# Define the folder path\n",
        "folder_path = '/content/drive/MyDrive/Phase 2/normal set 1'\n",
        "\n",
        "# Define the output folder path\n",
        "output_folder_path = '/content/drive/MyDrive/Phase 2/PP1'\n",
        "\n",
        "# Create the output folder if it doesn't exist\n",
        "!mkdir -p \"$output_folder_path\"\n",
        "\n",
        "# Function to perform noise removal using Gaussian blurring\n",
        "def remove_noise(image):\n",
        "    blurred = cv2.GaussianBlur(image, (5, 5), 0)\n",
        "    return blurred\n",
        "\n",
        "# Function to adjust brightness and contrast using gamma transformations\n",
        "def adjust_brightness_contrast(image, brightness=0.5, contrast=0.5):\n",
        "    adjusted = np.power(image / 255.0, 1 / contrast) * (255.0 * brightness)\n",
        "    return np.clip(adjusted, 0, 255).astype(np.uint8)\n",
        "\n",
        "# Function to sharpen the edges using unsharp masking\n",
        "def sharpen_edges(image):\n",
        "    blurred = cv2.GaussianBlur(image, (5, 5), 0)\n",
        "    sharpened = cv2.addWeighted(image, 1.5, blurred, -0.5, 0)\n",
        "    return sharpened\n",
        "\n",
        "# Function to resize the image to 512x512 without making it pixelated\n",
        "def resize_image(image):\n",
        "    resized = cv2.resize(image, (512, 512), interpolation=cv2.INTER_AREA)\n",
        "    return resized\n",
        "\n",
        "# Iterate over images in the folder\n",
        "for image_name in os.listdir(folder_path):\n",
        "    # Load the image\n",
        "    image_path = os.path.join(folder_path, image_name)\n",
        "    image = cv2.imread(image_path)\n",
        "\n",
        "    # Remove noise using Gaussian blurring\n",
        "    image = remove_noise(image)\n",
        "\n",
        "    # Adjust brightness and contrast using gamma transformations\n",
        "    image = adjust_brightness_contrast(image, brightness=1.2, contrast=1.5)\n",
        "\n",
        "    # Sharpen the edges using unsharp masking\n",
        "    image = sharpen_edges(image)\n",
        "\n",
        "    # Resize the image to 512x512\n",
        "    image = resize_image(image)\n",
        "\n",
        "    # Save the processed image\n",
        "    output_path = os.path.join(output_folder_path, image_name)\n",
        "    cv2.imwrite(output_path, image)\n"
      ],
      "metadata": {
        "id": "XNLdWXv9pbfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Masking pre-processed images"
      ],
      "metadata": {
        "id": "Qrk10FUmQ0PF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Define the curve points for the damage\n",
        "curve_points = np.array([[(100, 100), (200, 300), (400, 200), (500, 100)]], dtype=np.int32)\n",
        "\n",
        "# Set the input and output directories\n",
        "input_dir = '/content/drive/MyDrive/Phase 2/PP1-Combined'\n",
        "output_dir = '/content/drive/MyDrive/Phase 2/PP1-Combined Masked'\n",
        "\n",
        "# Create the output directory if it doesn't exist\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "# Loop through all the files in the input directory\n",
        "for filename in os.listdir(input_dir):\n",
        "    # Load the input image\n",
        "    img_path = os.path.join(input_dir, filename)\n",
        "    img = cv2.imread(img_path)\n",
        "\n",
        "    # Create a copy of the input image\n",
        "    damaged_img = img.copy()\n",
        "\n",
        "    # Draw many small curves on the damaged image\n",
        "    for i in range(10):\n",
        "        curve_points = np.random.randint(0, img.shape[0], size=(4, 2))\n",
        "        cv2.polylines(damaged_img, [curve_points], False, (255, 255, 255), thickness=7)\n",
        "\n",
        "    # Save the damaged image in the output directory\n",
        "    output_path = os.path.join(output_dir, filename)\n",
        "    cv2.imwrite(output_path, damaged_img)"
      ],
      "metadata": {
        "id": "qoIuOWWnuElo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sub-image Generation"
      ],
      "metadata": {
        "id": "LsRBSjU--blK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import imageio\n",
        "from PIL import Image\n",
        "\n",
        "# Function to upscale and upsize the image\n",
        "def upscale_image(image_path, output_path):\n",
        "    image = Image.fromarray(image_path)\n",
        "    upscaled_image = image.resize((512, 512), Image.BICUBIC)\n",
        "    upscaled_image.save(output_path)\n",
        "\n",
        "# Function to divide the image into four equal parts\n",
        "def divide_image(image_path, output_folder):\n",
        "    image = imageio.imread(image_path)\n",
        "    height, width, _ = image.shape\n",
        "    half_width = width // 2\n",
        "    half_height = height // 2\n",
        "\n",
        "    # Dividing the image into four parts\n",
        "    top_left = image[:half_height, :half_width]\n",
        "    top_right = image[:half_height, half_width:]\n",
        "    bottom_left = image[half_height:, :half_width]\n",
        "    bottom_right = image[half_height:, half_width:]\n",
        "\n",
        "    # Creating output folder if it doesn't exist\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "    # Get the original filename without extension\n",
        "    filename = os.path.splitext(os.path.basename(image_path))[0]\n",
        "\n",
        "    # Upscaling and saving each divided image\n",
        "    upscale_image(top_left, os.path.join(output_folder, f'{filename}_top_left.png'))\n",
        "    upscale_image(top_right, os.path.join(output_folder, f'{filename}_top_right.png'))\n",
        "    upscale_image(bottom_left, os.path.join(output_folder, f'{filename}_bottom_left.png'))\n",
        "    upscale_image(bottom_right, os.path.join(output_folder, f'{filename}_bottom_right.png'))\n",
        "\n",
        "# Path to the folder containing input images\n",
        "input_folder = '/content/drive/MyDrive/Phase 2/PP1'\n",
        "\n",
        "# Path to the output folder for the upscaled images\n",
        "output_folder = '/content/drive/MyDrive/Phase 2/PP1-SIG'\n",
        "\n",
        "# Iterate over each image in the input folder\n",
        "for filename in os.listdir(input_folder):\n",
        "    if filename.endswith('.png') or filename.endswith('.jpg'):\n",
        "        image_path = os.path.join(input_folder, filename)\n",
        "        divide_image(image_path, output_folder)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WdNoLe5aHJ8-",
        "outputId": "3ed5e7e3-e765-4200-8736-d93d25bfa0d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-e384ab439bbb>:13: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
            "  image = imageio.imread(image_path)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Training"
      ],
      "metadata": {
        "id": "C42qGth6CFoP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import binary_crossentropy\n",
        "from tensorflow.image import psnr as psnr_func\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "def partial_conv_block(x, channels, kernel_size=3, strides=1, padding='same', use_bias=False, alpha=0.2, use_bn=True):\n",
        "    # Define a partial convolutional layer with mask\n",
        "    x_in = x\n",
        "    x = Conv2D(channels, kernel_size, strides=strides, padding=padding, use_bias=use_bias)(x)\n",
        "    if use_bn:\n",
        "        x = BatchNormalization()(x)\n",
        "    mask = Conv2D(channels, kernel_size, strides=strides, padding=padding, use_bias=use_bias,\n",
        "                  activation='sigmoid')(x_in)\n",
        "    # Compute the element-wise multiplication between output feature maps and mask\n",
        "    x_out = multiply([x, mask])\n",
        "    # Apply LeakyReLU activation function\n",
        "    x_out = LeakyReLU(alpha=alpha)(x_out)\n",
        "    return x_out, mask\n",
        "\n",
        "def psnr(y_true, y_pred):\n",
        "    # Define a custom PSNR metric function\n",
        "    return psnr_func(y_true, y_pred, max_val=1.0)\n",
        "\n",
        "def unet_part_conv(input_shape, alpha=0.2):\n",
        "    # Defines the U-Net architecture with partial convolution blocks\n",
        "    inputs = Input(input_shape)\n",
        "\n",
        "    # Encoder\n",
        "    enc1, mask1 = partial_conv_block(inputs, 64, use_bn=False)\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(mask1)\n",
        "    enc2, mask2 = partial_conv_block(pool1, 128, use_bn=False)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(mask2)\n",
        "    enc3, mask3 = partial_conv_block(pool2, 256, use_bn=False)\n",
        "    pool3 = MaxPooling2D(pool_size=(2, 2))(mask3)\n",
        "    enc4, mask4 = partial_conv_block(pool3, 512, use_bn=False)\n",
        "    pool4 = MaxPooling2D(pool_size=(2, 2))(mask4)\n",
        "\n",
        "    # Bottleneck\n",
        "    center, mask5 = partial_conv_block(pool4, 1024, use_bn=True)\n",
        "\n",
        "    # Decoder with skip connections\n",
        "    up1 = Conv2DTranspose(512, 2, strides=2, padding='same')(center)\n",
        "    concat1 = concatenate([up1, enc4])\n",
        "    dec1, mask6 = partial_conv_block(concat1, 512, use_bn=True)\n",
        "    up2 = Conv2DTranspose(256, 2, strides=2, padding='same')(dec1)\n",
        "    concat2 = concatenate([up2, enc3])\n",
        "    dec2, mask7 = partial_conv_block(concat2, 256, use_bn=True)\n",
        "    up3 = Conv2DTranspose(128, 2, strides=2, padding='same')(dec2)\n",
        "    concat3 = concatenate([up3, enc2])\n",
        "    dec3, mask8 = partial_conv_block(concat3, 128, use_bn=True)\n",
        "    up4 = Conv2DTranspose(64, 2, strides=2, padding='same')(dec3)\n",
        "    concat4 = concatenate([up4, enc1])\n",
        "    dec4, mask9 = partial_conv_block(concat4, 64, use_bn=True)\n",
        "\n",
        "    # Output layer\n",
        "    outputs = Conv2D(1, 1, activation='sigmoid')(dec4)\n",
        "\n",
        "    # Define model and compile with loss and optimizer\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(optimizer=Adam(learning_rate=0.00002),\n",
        "                  loss=binary_crossentropy,\n",
        "                  metrics=['acc', psnr])\n",
        "    return model\n",
        "\n",
        "# Define the path to the directory containing the images\n",
        "img_dir = '/content/drive/MyDrive/Combined1/'\n",
        "\n",
        "# Get a list of all image files in the directory\n",
        "img_files = [os.path.join(img_dir, f) for f in os.listdir(img_dir) if f.endswith('.jpg')]\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "val_split = 0.2\n",
        "num_val_samples = int(val_split * len(img_files))\n",
        "np.random.shuffle(img_files)\n",
        "train_files = img_files[:-num_val_samples]\n",
        "val_files = img_files[-num_val_samples:]\n",
        "\n",
        "# Load training images and masks\n",
        "train_images = []\n",
        "train_masks = []\n",
        "for img_file in train_files:\n",
        "    # Load input image and mask\n",
        "    img = tf.io.decode_image(tf.io.read_file(img_file))\n",
        "    mask_file = os.path.join('/content/drive/MyDrive/CombinedMasked1/', os.path.basename(img_file))\n",
        "    mask = tf.io.decode_image(tf.io.read_file(mask_file))\n",
        "\n",
        "    # Reshape input image and mask as per the model requirements\n",
        "    img = tf.image.resize(img, (512, 512))\n",
        "    mask = tf.image.resize(mask, (512, 512))\n",
        "    mask = tf.image.rgb_to_grayscale(mask)\n",
        "    mask = tf.cast(mask, tf.float32) / 255.\n",
        "\n",
        "    # Add image and mask to list\n",
        "    train_images.append(img.numpy())\n",
        "    train_masks.append(mask.numpy())\n",
        "\n",
        "# Load validation images and masks\n",
        "val_images = []\n",
        "val_masks = []\n",
        "for img_file in val_files:\n",
        "    # Load input image and mask\n",
        "    img = tf.io.decode_image(tf.io.read_file(img_file))\n",
        "    mask_file = os.path.join('/content/drive/MyDrive/CombinedMasked1/', os.path.basename(img_file))\n",
        "    mask = tf.io.decode_image(tf.io.read_file(mask_file))\n",
        "\n",
        "    # Reshape input image and mask as per the model requirements\n",
        "    img = tf.image.resize(img, (512, 512))\n",
        "    mask = tf.image.resize(mask, (512, 512))\n",
        "    mask = tf.image.rgb_to_grayscale(mask)\n",
        "    mask = tf.cast(mask, tf.float32) / 255.\n",
        "\n",
        "    # Add image and mask to list\n",
        "    val_images.append(img.numpy())\n",
        "    val_masks.append(mask.numpy())\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "train_images = np.array(train_images)\n",
        "train_masks = np.array(train_masks)\n",
        "val_images = np.array(val_images)\n",
        "val_masks = np.array(val_masks)\n",
        "\n",
        "# Instantiate the model\n",
        "model = unet_part_conv((512, 512, 3))\n",
        "\n",
        "# Define the directory to save models in Google Drive\n",
        "save_dir = '/content/drive/MyDrive/model6/'\n",
        "\n",
        "# Create the save directory if it doesn't exist\n",
        "if not os.path.exists(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "\n",
        "# Define the callback to save the model after each epoch\n",
        "save_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    save_dir + 'model_{epoch:03d}.h5')\n",
        "\n",
        "# Fit the model to the training data and validate on the validation data\n",
        "model.fit(train_images, train_masks, epochs=20,\n",
        "          callbacks=[save_callback], batch_size=7,\n",
        "          validation_data=(val_images, val_masks))"
      ],
      "metadata": {
        "id": "dxY_rPIF-pqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting of oversized damaged images"
      ],
      "metadata": {
        "id": "aqi-dVTnCeJ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the path to the directory containing the images to split\n",
        "image_dir = '/content/drive/MyDrive/Datasets/Damaged/Damaged Mask/'\n",
        "# Define the path to the directory to save the split images\n",
        "output_dir = '/content/drive/MyDrive/Phase 2/split_images1'\n",
        "\n",
        "# Define the size of the patches\n",
        "patch_size = 512\n",
        "\n",
        "# Loop over each image in the input directory\n",
        "for filename in os.listdir(image_dir):\n",
        "    # Load the image\n",
        "    img = cv2.imread(os.path.join(image_dir, filename))\n",
        "\n",
        "    # Get the dimensions of the image\n",
        "    height, width, channels = img.shape\n",
        "\n",
        "    # Calculate the number of rows and columns of patches\n",
        "    rows = height // patch_size\n",
        "    cols = width // patch_size\n",
        "\n",
        "    # Loop over each patch and save it as a separate image\n",
        "    for i in range(rows):\n",
        "        for j in range(cols):\n",
        "            # Calculate the coordinates of the patch\n",
        "            x = j * patch_size\n",
        "            y = i * patch_size\n",
        "\n",
        "            # Extract the patch from the image\n",
        "            patch = img[y:y+patch_size, x:x+patch_size]\n",
        "\n",
        "            # Save the patch to disk\n",
        "            output_filename = os.path.splitext(filename)[0] + f'_{i}_{j}.jpg'\n",
        "            cv2.imwrite(os.path.join(output_dir, output_filename), patch)"
      ],
      "metadata": {
        "id": "MY5VvkGUhGBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing"
      ],
      "metadata": {
        "id": "jYJOzbMnh863"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import h5py\n",
        "import cv2\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.image import psnr as psnr_func\n",
        "\n",
        "# Define the custom PSNR metric function\n",
        "def psnr(y_true, y_pred):\n",
        "    max_pixel_value = 255.0\n",
        "    mse = tf.reduce_mean(tf.square(y_true - y_pred))\n",
        "    psnr = 20 * tf.math.log(max_pixel_value / tf.math.sqrt(mse)) / tf.math.log(10.0)\n",
        "    return psnr\n",
        "\n",
        "# Register the custom metric with Keras\n",
        "tf.keras.metrics.psnr = psnr\n",
        "\n",
        "psnr_values = []\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive',force_remount=True)\n",
        "\n",
        "# Set the input and output file paths\n",
        "input_dir = '/content/drive/MyDrive/Phase 2/split_images1'\n",
        "output_dir = '/content/drive/MyDrive/Phase 2/Inpainted'\n",
        "model_path = '/content/drive/MyDrive/model8/model_020.h5'\n",
        "\n",
        "# Load the Keras model and custom PSNR function\n",
        "with tf.keras.utils.custom_object_scope({'psnr': psnr}):\n",
        "    model = tf.keras.models.load_model(model_path)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['psnr'])\n",
        "\n",
        "# Loop over all the image files in the input directory\n",
        "for filename in os.listdir(input_dir):\n",
        "    # Load the image\n",
        "    img = cv2.imread(os.path.join(input_dir, filename))\n",
        "\n",
        "    # Convert the image to grayscale\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "    # Threshold the image to obtain a mask of the white areas\n",
        "    mask = cv2.threshold(gray, 240, 255, cv2.THRESH_BINARY)[1]\n",
        "\n",
        "    # Inpaint the white areas using the cv2.INPAINT_TELEA method\n",
        "    inpaint = cv2.inpaint(img, mask, 3, cv2.INPAINT_TELEA)\n",
        "\n",
        "    # Predict the output using the loaded model\n",
        "    output = model.predict(tf.expand_dims(inpaint, axis=0))[0]\n",
        "\n",
        "    cv2.imwrite(os.path.join(output_dir, filename[:-4] + '.jpg'), inpaint)"
      ],
      "metadata": {
        "id": "xrh5aCMZdQr3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}